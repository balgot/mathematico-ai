{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training CNN *AlphaZero* Agent\n",
    "\n",
    "This notebook contains configuration and code to train a neural network, \n",
    "which is integrated with a `Mathematico` agent to play the game.\n",
    "\n",
    "Consider an agent **A** which is using MCTS and neural network *N* to find the\n",
    "best move.\n",
    "\n",
    "This algorithm works as follows: \n",
    "\n",
    "* play *M* games, recording the whole MCTS tree including all statistics and \n",
    "the final outcome (in the expert memory).\n",
    "* sample *k* moves from the memory (experience replay), compute the loss between \n",
    "*N(s)* and expected reward and update the params of the network. \n",
    "\n",
    "The architecture of the neural network is inspired by *LeelaZero*, and consists\n",
    "of convolutional blocks packed in residual layers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains configuration and code to train a neural network, which is integrated with a `Mathematico` agent to play the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"name\": \"leela-arch\",  # wandb-run name\n",
    "    \"cuda\": False,  # use CUDA if possible (beware of memory)    \n",
    "    \"seed\": 0,    \n",
    "    \"lr\": 0.005,      \n",
    "    \"n_simulated_games\": 20,  # at least 2 for stddev to exist\n",
    "    \"simuls_limit\": 20, # per move\n",
    "    \"policy repeats\": 1, # how many times to rerun the rollout policy\n",
    "    \"static_policy\": True, # if True, policy just returns the value of the node\n",
    "    \"sample\": True,  # do random sampling from data or just shuffle, the only option due to RAM contraints\n",
    "    \"batch_size\": 8,  # only applicable if \"sample\" = True\n",
    "    \"n_training_loops\": 2, # per one RL epoch\n",
    "    \"n_epochs\": 200\n",
    "}\n",
    "\n",
    "WANDB_PROJECT_NAME = None\n",
    "assert WANDB_PROJECT_NAME is not None, \"please provide w&b project name\"\n",
    "assert config[\"n_simulated_games\"] > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.abspath(os.path.join(\"../\")))\n",
    "\n",
    "import random\n",
    "import statistics\n",
    "from copy import deepcopy\n",
    "import time\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from torchview import draw_graph\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import wandb\n",
    "wandb.init(\n",
    "    config=config, \n",
    "    project=WANDB_PROJECT_NAME, \n",
    "    name=config.get(\"name\", None), \n",
    "    settings=wandb.Settings(start_method=\"fork\")\n",
    ")\n",
    "\n",
    "import graphviz\n",
    "graphviz.set_jupyter_format('png')  # VS code fix for cropped images from torchview\n",
    "\n",
    "import mathematico\n",
    "from src.utils import mcts\n",
    "from src.utils.extract_data import extract\n",
    "from src.utils.symmetries import all_symmetries\n",
    "import src.nets as nets\n",
    "from src.utils.lr import display_learning_rate\n",
    "\n",
    "\n",
    "########################################\n",
    "# random seed\n",
    "########################################\n",
    "\n",
    "torch.random.manual_seed(config[\"seed\"])\n",
    "random.seed(config[\"seed\"])\n",
    "np.random.seed(config[\"seed\"])\n",
    "\n",
    "\n",
    "########################################\n",
    "# cuda settings\n",
    "########################################\n",
    "\n",
    "if not config[\"cuda\"]:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "dev"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Definition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the input, the network takes `list[list[int]]` - the board and it approximates the value function `V(s) = V(board)`.\n",
    "\n",
    "All inputs are one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.nets import LeelaNetwork\n",
    "model = LeelaNetwork(5, 64, 6).to(dev)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "\n",
    "# create fake input for testing the net and displaying the info\n",
    "_board_batch = torch.tensor([[[0, 1, 11, 12, 13]] * 5] * 32, device=dev)\n",
    "_out = model.forward(_board_batch)\n",
    "summary(model, [(5, 5)], depth=7);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.agents.mcts_player import MctsPlayer\n",
    "\n",
    "def policy_static(state: mcts.StateI) -> float:\n",
    "    board = torch.tensor([state.board.grid], device=dev)\n",
    "    return model(board)\n",
    "\n",
    "\n",
    "def policy_dynamic(state: mcts.StateI) -> float:\n",
    "    _board = deepcopy(state.board)\n",
    "    _possible_moves = set(_board.possible_moves())\n",
    "    _deck = [k for k, v in state.deck.items() for _ in range(v)]\n",
    "    random.shuffle(_deck)\n",
    "    \n",
    "    def mmove(move, card):\n",
    "        b = deepcopy(_board.grid)\n",
    "        b[move[0]][move[1]] = card\n",
    "        return b\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(len(_possible_moves)):\n",
    "            batch = torch.tensor([\n",
    "                mmove(move, _deck[i]) for move in _possible_moves\n",
    "            ], device=dev)\n",
    "            out = model(batch)\n",
    "            idx = torch.argmax(out)\n",
    "            move = list(_possible_moves)[idx]\n",
    "            _board.make_move(move, _deck[i])\n",
    "            _possible_moves.discard(move)\n",
    "        return _board.score()\n",
    "            \n",
    "        \n",
    "        \n",
    "def repeated_dynamic(state):\n",
    "    total = 0\n",
    "    REPS = config['policy repeats']\n",
    "    for _ in range(REPS):\n",
    "        total += policy_dynamic(state)\n",
    "    return total / REPS     \n",
    "\n",
    "\n",
    "agent = MctsPlayer(\n",
    "    max_time_ms=None, \n",
    "    max_simulations=config[\"simuls_limit\"], \n",
    "    policy=policy_static if config[\"static_policy\"] else repeated_dynamic\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check it works by playing a random game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "start = time.time()\n",
    "arena = mathematico.Arena()\n",
    "arena.add_player(agent)\n",
    "arena.run(seed=0, rounds=1, verbose=True)\n",
    "end = time.time()\n",
    "\n",
    "per_move_sec = (end - start) / (5 * 5)\n",
    "print(f\"{per_move_sec=}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _log(start_time, mean, std, min_score, max_score, it, loss, loss_mcts, loss_final):\n",
    "    # assert torch.isclose(loss_mcts + loss_final + loss_max, loss)\n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    log_dict = {\n",
    "        \"time\": duration,\n",
    "        \"mean\": mean,\n",
    "        \"std\": std,\n",
    "        \"min score\": min_score,\n",
    "        \"max score\": max_score,\n",
    "        \"loss\": loss,\n",
    "        \"loss [mcts]\": loss_mcts,\n",
    "        \"loss [final]\": loss_final,\n",
    "        \"lr\": optimizer.param_groups[0]['lr']  # valid only with one param group for optimizer\n",
    "    }\n",
    "        \n",
    "    wandb.log(log_dict)\n",
    "\n",
    "\n",
    "\n",
    "def learn_episode(agent: MctsPlayer, model: torch.nn.Module, n_games, batch_size, m_training):\n",
    "    expert_memory = []   \n",
    "    \n",
    "    # for logging..\n",
    "    _scores = []\n",
    "    loss = 0\n",
    "    loss_mcts = 0\n",
    "    loss_final = 0\n",
    "\n",
    "    #############################################################################\n",
    "    #                           playing phase\n",
    "    #############################################################################\n",
    "    \n",
    "    model.eval()\n",
    "    for game in trange(n_games, desc=\"Game playing phase\", leave=None, position=1):\n",
    "        agent.reset()\n",
    "        cards = [i for i in range(1, 13+1) for _ in range(4)]\n",
    "        random.shuffle(cards)\n",
    "\n",
    "        # game memory - all states visited during mcts\n",
    "        game_memory = []\n",
    "        \n",
    "        # which states were actually played\n",
    "        true_states = []\n",
    "\n",
    "        # play all the moves till the end\n",
    "        for move in trange(5*5, desc=\"Playing moves\", leave=None, position=2):\n",
    "            state = deepcopy(agent.board.grid)\n",
    "            card = cards[move]\n",
    "            estimate, root = agent.move_(card)\n",
    "            visited_states = extract(root)\n",
    "            for b, e, v, d, h in visited_states:\n",
    "                for s in all_symmetries(b):\n",
    "                    game_memory.append((s, e, v, d, h))\n",
    "            true_states.append(visited_states[0])\n",
    "\n",
    "        final_score = agent.board.score()\n",
    "        _scores.append(final_score)\n",
    "        \n",
    "        for board, exp, visits, depth, height in true_states:\n",
    "            for s in all_symmetries(board):\n",
    "                expert_memory.append((s, final_score, exp, visits, depth, height))\n",
    "        for b, e, v, d, h in game_memory:\n",
    "            expert_memory.append((b, None, e, v, d, h))\n",
    "\n",
    "            \n",
    "    #############################################################################\n",
    "    #                           training phase\n",
    "    #############################################################################\n",
    "    \n",
    "    model.train()\n",
    "    weights = [entry[3] for entry in expert_memory]  # visit counts\n",
    "    _s = sum(weights)\n",
    "    weights = [w/_s for w in weights]\n",
    "    indices = np.random.choice(len(expert_memory), size=(m_training, batch_size), replace=m_training*batch_size > len(expert_memory), p=weights)\n",
    "    \n",
    "    for train in trange(m_training, desc=\"Training loop\", leave=None, position=1):\n",
    "        batch = [expert_memory[idx] for idx in indices[train]]\n",
    "        \n",
    "        for with_final in (True, False):  # two passes, one for played states, one for hypothetical        \n",
    "            boards, reals, exps, viss, deps, heis = [], [], [], [], [], []\n",
    "            for b, real, exp, vis, dep, hei in batch:\n",
    "                if (real is not None) == with_final:\n",
    "                    boards.append(b)\n",
    "                    reals.append(real)\n",
    "                    exps.append(exp)\n",
    "                    viss.append(vis)\n",
    "                    deps.append(dep)\n",
    "                    heis.append(hei)\n",
    "\n",
    "            if boards:                \n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outs = torch.squeeze(model(torch.tensor(boards, device=dev)), dim=1)\n",
    "                target = torch.tensor(exps, device=dev)\n",
    "                \n",
    "                _mcts_loss_norm = torch.log(torch.tensor(viss, device=dev) + 1) / (torch.tensor(heis, device=dev) + 1)\n",
    "                _mcts_loss = torch.mean(_mcts_loss_norm * (target - outs)**2)\n",
    "                \n",
    "                _final_coef = (math.log(2) + 1) / (1 + torch.log(1 + torch.tensor(heis, device=dev)))\n",
    "                _final_loss = 0 if not with_final else torch.mean(_final_coef * _mcts_loss_norm * (torch.tensor(reals, device=dev) - outs)**2)\n",
    "                \n",
    "                _loss = _mcts_loss + _final_loss\n",
    "                \n",
    "                if torch.any(torch.isnan(_loss)):\n",
    "                    raise RuntimeError(\"NaN detected, instable learning...\" + f\"{_mcts_loss_norm=}\\t{_mcts_loss=}\\t{_final_loss=}\")\n",
    "                    \n",
    "                _loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    loss += _loss / m_training\n",
    "                    loss_mcts += _mcts_loss / m_training\n",
    "                    if with_final:\n",
    "                        loss_final += _final_loss / m_training\n",
    "            \n",
    "    scheduler.step(loss_mcts)\n",
    "    \n",
    "    return (\n",
    "        statistics.mean(_scores), \n",
    "        statistics.stdev(_scores),\n",
    "        min(_scores),\n",
    "        max(_scores),\n",
    "        loss,\n",
    "        loss_mcts,\n",
    "        loss_final\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "START = time.time()\n",
    "\n",
    "# _log(START, None, None, None, None, 0, None, None, None)\n",
    "for epoch in trange(1, 1+config[\"n_epochs\"], desc=\"Epochs\"):\n",
    "    mean, std, mini, maxi, L, Lm, Lf = learn_episode(\n",
    "        agent, \n",
    "        model, \n",
    "        n_games=config[\"n_simulated_games\"], \n",
    "        batch_size=config[\"batch_size\"], \n",
    "        m_training=config[\"n_training_loops\"]\n",
    "    ) \n",
    "    \n",
    "    \n",
    "    \n",
    "    if torch.any(torch.isnan(L)) or torch.any(torch.isnan(Lm)):\n",
    "        print(\"Instabilities (NaN), aborting...\")\n",
    "        break\n",
    "    _log(START, mean, std, mini, maxi, epoch, L, Lm, Lf)\n",
    "    \n",
    "    # form of early stopping\n",
    "    if optimizer.param_groups[0]['lr'] < 1e-8:\n",
    "        print(f\"[{epoch=}] Learning rate is too low, aborting...\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save & Load Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the loaded network\n",
    "torch.save(model.state_dict(), \"leela-64-6.pt\")\n",
    "\n",
    "# load the saved model\n",
    "new_model = LeelaNetwork(5, 64, 6).to(dev)\n",
    "new_model.load_state_dict(torch.load(\"leela-64-6.pt\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

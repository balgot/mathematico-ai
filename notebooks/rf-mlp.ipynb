{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training MLP *AlphaZero* Agent\n",
    "\n",
    "This notebook contains configuration and code to train a neural network, \n",
    "which is integrated with a `Mathematico` agent to play the game.\n",
    "\n",
    "Consider an agent **A** which is using MCTS and neural network *N* to find the\n",
    "best move.\n",
    "\n",
    "This algorithm works as follows: \n",
    "\n",
    "* play *M* games, recording the whole MCTS tree including all statistics and \n",
    "the final outcome (in the expert memory).\n",
    "* sample *k* moves from the memory (experience replay), compute the loss between \n",
    "*N(s)* and expected reward and update the params of the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"algorithm\": \"recreate fasst 208\",  # rough description of the algorithm\n",
    "    \"name\": \"more-batch-samples\",  # wandb-run name\n",
    "    \"cuda\": True,  # use CUDA if possible (beware of memory)\n",
    "    \n",
    "    # random related, due to \n",
    "    \"seed\": 0,\n",
    "    \"test_seed\": 42,  # used for always measuring performance on the same games\n",
    "    \"test after . epochs\": 500,  # how many epochs to train before conducting a tournament between agents\n",
    "    \n",
    "    # neural net\n",
    "    \"network\": \"Simple_Board_v0\",  # name of the network to use\n",
    "    \n",
    "    ## pretraining on only final states, uses exponential decay\n",
    "    \"pretrain\": {  # None for no pretraining\n",
    "        \"epochs\": 1024,\n",
    "        \"samples\": 128,\n",
    "        \"start-lr\": 0.001,\n",
    "        \"weight-decay\": 0,  # adam weight decay\n",
    "    },\n",
    "    \n",
    "    ## Optimizer params\n",
    "    \"optimizer\": \"Adam\",  # the only option\n",
    "    \"lr\": 0.005,  # initial learning rate\n",
    "    \"betas\": (0.9, 0.999),\n",
    "    \"weight-decay\": 0,\n",
    "    \n",
    "    ## LR scheduler params\n",
    "    \"lr-scheduler\": \"ExponentialLR\", # only option for now (used with ReduceLROnPlateau)\n",
    "    \"lr-gamma\": 0.96,  \n",
    "    \n",
    "    ## gradient clipping\n",
    "    \"max-gradient-norm\": 1,\n",
    "    \n",
    "    # about MCTS\n",
    "    \"stochastic\": False,  # the only option now is deterministic\n",
    "    \"time_limit\": None,    # milliseconds\n",
    "    \"simuls_limit\": 20, # per move\n",
    "    \"policy repeats\": 1, # how many times to rerun the rollout policy\n",
    "    \"static_policy\": False, # if True, policy just returns the value of the node\n",
    "    \n",
    "    # algo params\n",
    "    \"test_games\": 10,\n",
    "    \"n_simulated_games\": 20,  # at least 2 for stddev to exist\n",
    "    \"sample\": True,  # do random sampling from data or just shuffle, the only option due to RAM contraints\n",
    "    \"batch_size\": 8,  # only applicable if \"sample\" = True\n",
    "    \"n_training_loops\": 4, # per one RL epoch\n",
    "    \"n_epochs\": 200,\n",
    "\n",
    "    # loss function calculation\n",
    "    \"alpha\": 1,  # how well do we approximate MCTS\n",
    "    \"beta\": 0,  # discounted final score approximation\n",
    "    \"target-scale\": 1.05,  # make neural network to overpredict by .5%\n",
    "}\n",
    "\n",
    "WANDB_PROJECT_NAME = None\n",
    "assert WANDB_PROJECT_NAME is not None, \"please provide w&b project name\"\n",
    "assert config[\"optimizer\"] == \"Adam\"\n",
    "assert config[\"lr-scheduler\"] == \"ExponentialLR\"\n",
    "assert config[\"n_simulated_games\"] > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.abspath(os.path.join(\"../\")))\n",
    "\n",
    "import random\n",
    "import statistics\n",
    "from copy import deepcopy\n",
    "import time\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from torchview import draw_graph\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import graphviz\n",
    "graphviz.set_jupyter_format('png')  # VS code fix for cropped images from torchview\n",
    "\n",
    "import mathematico\n",
    "from src.utils import mcts\n",
    "from src.utils.extract_data import extract\n",
    "from src.utils.symmetries import all_symmetries\n",
    "import src.nets as nets\n",
    "from src.utils.lr import display_learning_rate\n",
    "\n",
    "\n",
    "########################################\n",
    "# random seed\n",
    "########################################\n",
    "\n",
    "torch.random.manual_seed(config[\"seed\"])\n",
    "random.seed(config[\"seed\"])\n",
    "np.random.seed(config[\"seed\"])\n",
    "\n",
    "\n",
    "########################################\n",
    "# cuda settings\n",
    "########################################\n",
    "\n",
    "if not config[\"cuda\"]:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "dev = torch.device(\"cuda\") if torch.cuda.is_available() and config[\"cuda\"] else torch.device(\"cpu\")\n",
    "dev"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight & Biases Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init(config=config, project=WANDB_PROJECT_NAME, name=config.get(\"name\", None), settings=wandb.Settings(start_method=\"fork\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Definition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the input, the network takes `list[list[int]]` - the board and it approximates the value function `V(s) = V(board)`.\n",
    "\n",
    "All inputs are one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_nn_cls = getattr(nets, config[\"network\"])\n",
    "model = _nn_cls().to(dev)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"], betas=config[\"betas\"], weight_decay=config[\"weight-decay\"])\n",
    "scheduler1 = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=config[\"lr-gamma\"])\n",
    "scheduler2 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "\n",
    "# create fake input for testing the net and displaying the info\n",
    "_board_batch = torch.tensor([[[0, 1, 11, 12, 13]] * 5] * 32, device=dev)\n",
    "_out = model.forward(_board_batch)\n",
    "summary(model, [(5, 5)], depth=7);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "draw_graph(model, input_data=_board_batch, depth=3, graph_dir=\"LR\").visual_graph"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.agents.mcts_player import MctsPlayer, CardState, MoveState\n",
    "\n",
    "def policy_static(state: mcts.StateI) -> float:\n",
    "    board = torch.tensor([state.board.grid], device=dev)\n",
    "    return model(board)\n",
    "\n",
    "\n",
    "def policy_dynamic(state: mcts.StateI) -> float:\n",
    "    _board = deepcopy(state.board)\n",
    "    _possible_moves = set(_board.possible_moves())\n",
    "    _deck = [k for k, v in state.deck.items() for _ in range(v)]\n",
    "    random.shuffle(_deck)\n",
    "    \n",
    "    def mmove(move, card):\n",
    "        b = deepcopy(_board.grid)\n",
    "        b[move[0]][move[1]] = card\n",
    "        return b\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(len(_possible_moves)):\n",
    "            batch = torch.tensor([\n",
    "                mmove(move, _deck[i]) for move in _possible_moves\n",
    "            ], device=dev)\n",
    "            out = model(batch)\n",
    "            idx = torch.argmax(out)\n",
    "            move = list(_possible_moves)[idx]\n",
    "            _board.make_move(move, _deck[i])\n",
    "            _possible_moves.discard(move)\n",
    "        return _board.score()\n",
    "            \n",
    "        \n",
    "        \n",
    "def repeated_dynamic(state):\n",
    "    total = 0\n",
    "    REPS = config['policy repeats']\n",
    "    for _ in range(REPS):\n",
    "        total += policy_dynamic(state)\n",
    "    return total / REPS       \n",
    "    \n",
    "\n",
    "\n",
    "agent = MctsPlayer(\n",
    "    max_time_ms=config[\"time_limit\"], \n",
    "    max_simulations=config[\"simuls_limit\"], \n",
    "    policy=policy_static if config[\"static_policy\"] else repeated_dynamic\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check it works by playing a random game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "start = time.time()\n",
    "arena = mathematico.Arena()\n",
    "arena.add_player(agent)\n",
    "arena.run(seed=0, rounds=1, verbose=True)\n",
    "end = time.time()\n",
    "\n",
    "per_move_sec = (end - start) / (5 * 5)\n",
    "print(f\"{per_move_sec=}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VNPlayer(mathematico.Player):\n",
    "    def reset(self):\n",
    "        self.board = mathematico.Board()\n",
    "        \n",
    "    def move(self, card: int) -> None:\n",
    "        possibles = list(self.board.possible_moves())\n",
    "        \n",
    "        def place(row, col):\n",
    "            _g = deepcopy(self.board.grid)\n",
    "            _g[row][col] = card\n",
    "            return _g\n",
    "        \n",
    "        batch = torch.tensor([place(row, col) for row, col in possibles], device=dev)\n",
    "        scores = model(batch)\n",
    "        idx = torch.argmax(scores)\n",
    "        row, col = possibles[idx]\n",
    "        self.board.make_move((row, col), card)\n",
    "\n",
    "\n",
    "def _eval_against_players(agent, model, rounds):\n",
    "    \"\"\"\n",
    "    Evaluate the agent against:\n",
    "        1. random player\n",
    "        2. mcts player with same time for a game (not very precise..)\n",
    "        3. mcts with same number of simuls\n",
    "        4. only value network\n",
    "        \n",
    "    Returns:\n",
    "        win rate for 1, 2, 3, 4\n",
    "        avg score for 1, 2, 3, 4\n",
    "        descriptions\n",
    "        ranking value (avg number of player defeated)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    players = [\n",
    "        agent, \n",
    "        mathematico.RandomPlayer(), \n",
    "        MctsPlayer(max_time_ms=per_move_sec * 1000),\n",
    "        MctsPlayer(max_simulations=config[\"simuls_limit\"]),\n",
    "        VNPlayer()\n",
    "    ]\n",
    "    \n",
    "    desc = [\"vn+mcts player\", \"random\", \"mcts(time)\", \"mcts(simuls)\", \"value net\"]\n",
    "    \n",
    "    wins_agains = [0 for _ in players]\n",
    "    total_score = [0 for _ in players]\n",
    "    rank = 0\n",
    "    \n",
    "    for _round in trange(rounds, desc=\"Evaluating performance tournament\", leave=None):\n",
    "        game = mathematico.Mathematico(seed=_round + config[\"test_seed\"])\n",
    "        for player in players:\n",
    "            player.reset()\n",
    "            game.add_player(player)\n",
    "            \n",
    "        scores = game.play()\n",
    "        \n",
    "        # first is always our agent\n",
    "        for i in range(len(scores)):\n",
    "            if scores[0] >= scores[i]:\n",
    "                wins_agains[i] += 1\n",
    "                \n",
    "        for i in range(len(scores)):\n",
    "            total_score[i] += scores[i]\n",
    "            \n",
    "        rank += sum(scores[0] >= scores[i] for i in range(1, len(scores)))\n",
    "        \n",
    "    rank /= (rounds * (len(players) - 1))\n",
    "    return [w/rounds for w in wins_agains], [s/rounds for s in total_score], desc, rank\n",
    "\n",
    "\n",
    "def _log(start_time, mean, std, min_score, max_score, it, loss, loss_mcts, loss_final):\n",
    "    # assert torch.isclose(loss_mcts + loss_final + loss_max, loss)\n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    log_dict = {\n",
    "        \"time\": duration,\n",
    "        \"mean\": mean,\n",
    "        \"std\": std,\n",
    "        \"min score\": min_score,\n",
    "        \"max score\": max_score,\n",
    "        \"loss\": loss,\n",
    "        \"loss [mcts]\": loss_mcts,\n",
    "        \"loss [final]\": loss_final,\n",
    "        \"lr\": optimizer.param_groups[0]['lr']  # valid only with one param group for optimizer\n",
    "    }\n",
    "\n",
    "    if it % config[\"test after . epochs\"] == 0:\n",
    "        wins, scores, desc, rank = _eval_against_players(agent, model, rounds=config[\"test_games\"])\n",
    "        for d, w, s in zip(desc, wins, scores):\n",
    "            log_dict[d + \" wins %\"] = w\n",
    "            log_dict[d + \" [[avg score]]\"] = s\n",
    "        log_dict[\"rank\"] = rank\n",
    "        \n",
    "    wandb.log(log_dict)\n",
    "\n",
    "\n",
    "\n",
    "def learn_episode(agent: MctsPlayer, model: torch.nn.Module, n_games, batch_size, m_training):\n",
    "    expert_memory = []   \n",
    "    \n",
    "    # for logging..\n",
    "    _scores = []\n",
    "    loss = 0\n",
    "    loss_mcts = 0\n",
    "    loss_final = 0\n",
    "\n",
    "    #############################################################################\n",
    "    #                           playing phase\n",
    "    #############################################################################\n",
    "    \n",
    "    model.eval()\n",
    "    for game in trange(n_games, desc=\"Game playing phase\", leave=None, position=1):\n",
    "        agent.reset()\n",
    "        cards = [i for i in range(1, 13+1) for _ in range(4)]\n",
    "        random.shuffle(cards)\n",
    "\n",
    "        # game memory - all states visited during mcts\n",
    "        game_memory = []\n",
    "        \n",
    "        # which states were actually played\n",
    "        true_states = []\n",
    "\n",
    "        # play all the moves till the end\n",
    "        for move in trange(5*5, desc=\"Playing moves\", leave=None, position=2):\n",
    "            state = deepcopy(agent.board.grid)\n",
    "            card = cards[move]\n",
    "            estimate, root = agent.move_(card)\n",
    "            visited_states = extract(root)\n",
    "            for b, e, v, d, h in visited_states:\n",
    "                for s in all_symmetries(b):\n",
    "                    game_memory.append((s, e, v, d, h))\n",
    "            true_states.append(visited_states[0])\n",
    "\n",
    "        final_score = agent.board.score()\n",
    "        _scores.append(final_score)\n",
    "        \n",
    "        for board, exp, visits, depth, height in true_states:\n",
    "            for s in all_symmetries(board):\n",
    "                expert_memory.append((s, final_score, exp, visits, depth, height))\n",
    "        for b, e, v, d, h in game_memory:\n",
    "            expert_memory.append((b, None, e, v, d, h))\n",
    "\n",
    "            \n",
    "    #############################################################################\n",
    "    #                           training phase\n",
    "    #############################################################################\n",
    "    \n",
    "    model.train()\n",
    "    weights = [entry[3] for entry in expert_memory]  # visit counts\n",
    "    _s = sum(weights)\n",
    "    weights = [w/_s for w in weights]\n",
    "    indices = np.random.choice(len(expert_memory), size=(m_training, batch_size), replace=m_training*batch_size > len(expert_memory), p=weights)\n",
    "    \n",
    "    for train in trange(m_training, desc=\"Training loop\", leave=None, position=1):\n",
    "        batch = [expert_memory[idx] for idx in indices[train]]\n",
    "        \n",
    "        for with_final in (True, False):  # two passes, one for played states, one for hypothetical        \n",
    "            boards, reals, exps, viss, deps, heis = [], [], [], [], [], []\n",
    "            for b, real, exp, vis, dep, hei in batch:\n",
    "                if (real is not None) == with_final:\n",
    "                    boards.append(b)\n",
    "                    reals.append(real)\n",
    "                    exps.append(exp)\n",
    "                    viss.append(vis)\n",
    "                    deps.append(dep)\n",
    "                    heis.append(hei)\n",
    "\n",
    "            if boards:                \n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outs = torch.squeeze(model(torch.tensor(boards, device=dev)), dim=1)\n",
    "                target = config[\"target-scale\"] * torch.tensor(exps, device=dev)\n",
    "                \n",
    "                _mcts_loss_norm = torch.log(torch.tensor(viss, device=dev) + 1) / (torch.tensor(heis, device=dev) + 1)\n",
    "                _mcts_loss = torch.mean(config[\"alpha\"] * _mcts_loss_norm * (target - outs)**2)\n",
    "                \n",
    "                _final_coef = (math.log(2) + 1) / (1 + torch.log(1 + torch.tensor(heis, device=dev)))\n",
    "                _final_loss = 0 if not with_final else torch.mean(config[\"beta\"] * _final_coef * _mcts_loss_norm * (torch.tensor(reals, device=dev) - outs)**2)\n",
    "                \n",
    "                _loss = _mcts_loss + _final_loss\n",
    "                \n",
    "                if torch.any(torch.isnan(_loss)):\n",
    "                    raise RuntimeError(\"NaN detected, instable learning...\" + f\"{_mcts_loss_norm=}\\t{_mcts_loss=}\\t{_final_loss=}\")\n",
    "                    \n",
    "                _loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config[\"max-gradient-norm\"]) \n",
    "                optimizer.step()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    loss += _loss / m_training\n",
    "                    loss_mcts += _mcts_loss / m_training\n",
    "                    if with_final:\n",
    "                        loss_final += _final_loss / m_training\n",
    "            \n",
    "    scheduler1.step()  \n",
    "    scheduler2.step(loss_mcts)\n",
    "    \n",
    "    return (\n",
    "        statistics.mean(_scores), \n",
    "        statistics.stdev(_scores),\n",
    "        min(_scores),\n",
    "        max(_scores),\n",
    "        loss,\n",
    "        loss_mcts,\n",
    "        loss_final\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config[\"pretrain\"] is not None:\n",
    "    losses = []\n",
    "    lr = []\n",
    "    \n",
    "    o = torch.optim.Adam(model.parameters(), lr=config[\"pretrain\"][\"start-lr\"])\n",
    "    # s = torch.optim.lr_scheduler.ExponentialLR(o, gamma=config[\"pretrain\"][\"gamma\"])\n",
    "    s = torch.optim.lr_scheduler.ReduceLROnPlateau(o)\n",
    "    \n",
    "    for epoch in (pbar := trange(config[\"pretrain\"][\"epochs\"], desc=\"Pretraining\")):\n",
    "        boards = []\n",
    "        vals = []\n",
    "        for _ in range(config[\"pretrain\"][\"samples\"]):\n",
    "            deck = [i for i in range(1, 14) for _ in range(4)]\n",
    "            random.shuffle(deck)\n",
    "            b = mathematico.Board()\n",
    "            for r in range(5):\n",
    "                for c in range(5):\n",
    "                    b.make_move((r, c), deck[5*r + c])\n",
    "            boards.append(b.grid)\n",
    "            vals.append(b.score())\n",
    "\n",
    "        _loss_fn = torch.nn.MSELoss()\n",
    "        o.zero_grad()\n",
    "        outs = model(torch.tensor(boards, device=dev))\n",
    "        target = torch.unsqueeze(torch.tensor(vals, device=dev, dtype=outs.dtype), -1)\n",
    "        loss = _loss_fn(outs, target)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config[\"max-gradient-norm\"])\n",
    "        o.step()\n",
    "        s.step(loss)\n",
    "        \n",
    "        if torch.any(torch.isnan(loss)):\n",
    "            print(\"NaN detected...\")\n",
    "            break\n",
    "        \n",
    "        losses.append(loss.sum().detach().cpu().numpy())\n",
    "        pbar.set_description(f\"Pretraining (loss: {losses[-1]:.02f})\")\n",
    "        lr.append(o.param_groups[0]['lr'])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "ax = plt.gca()\n",
    "ax.set_yscale('log')\n",
    "ax.set_title(\"log-loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(lr)\n",
    "ax = plt.gca()\n",
    "ax.set_yscale('log')\n",
    "ax.set_title(\"learning rate\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how are the predictions on different boards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_set = [[1,2,3,4,5]]*4 + [[6,7,8,9,10]]\n",
    "_b = mathematico.Board()\n",
    "for r in range(5):\n",
    "    for c in range(5):\n",
    "        _b.make_move((r, c), _set[r][c])\n",
    "print(_b)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    print(f\"Real = {_b.score()}\\tPredicted = {model(torch.tensor([_set], device=dev)).cpu().numpy()[0][0]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for _ in range(25):\n",
    "    _b = mathematico.Board()\n",
    "    _deck = [k for k in range(1, 14) for _ in range(4)]\n",
    "    random.shuffle(_deck)\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            _b.make_move((i, j), _deck[5*i+j])\n",
    "\n",
    "    # print(_b)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        print(f\"Real = {_b.score()}\\tPredicted = {model(torch.tensor([_b.grid], device=dev)).cpu().numpy()[0][0]:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "START = time.time()\n",
    "\n",
    "# _log(START, None, None, None, None, 0, None, None, None)\n",
    "for epoch in trange(1, 1+config[\"n_epochs\"], desc=\"Epochs\"):\n",
    "    mean, std, mini, maxi, L, Lm, Lf = learn_episode(\n",
    "        agent, \n",
    "        model, \n",
    "        n_games=config[\"n_simulated_games\"], \n",
    "        batch_size=config[\"batch_size\"], \n",
    "        m_training=config[\"n_training_loops\"]\n",
    "    ) \n",
    "    \n",
    "    \n",
    "    \n",
    "    if torch.any(torch.isnan(L)) or torch.any(torch.isnan(Lm)):\n",
    "        print(\"Instabilities (NaN), aborting...\")\n",
    "        break\n",
    "    _log(START, mean, std, mini, maxi, epoch, L, Lm, Lf)\n",
    "    \n",
    "    # form of early stopping\n",
    "    if optimizer.param_groups[0]['lr'] < 1e-8:\n",
    "        print(f\"[{epoch=}] Learning rate is too low, aborting...\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save & Load Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the loaded network\n",
    "torch.save(model.state_dict(), config[\"network\"] + \".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved model\n",
    "new_model = nets.Simple_Board_v0().to(dev)\n",
    "new_model.load_state_dict(torch.load(\"Simple_Board_v0.pt\"))\n",
    "all(new_model.block[2]._parameters[\"bias\"] == model.block[2]._parameters[\"bias\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

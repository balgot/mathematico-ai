1) reduce deepcopy
    a) in rollout random, dont create new states, just random shuffle possible actions, do N times make move, eval, N times unmake move
    b) dont keep the whole board in each state, keep only move, when evaluating backtrack
    c) reuse states from previous round

2) model also variance (in addition to expected value) and use it as uncertainty for the leaf
    - will need to keep also a list of achieved rewards for each state to approximate it

3) epsilon greedy approach

4) unsupervised learning
    in the beginning, just encode the board and then take the encoded representation
    then freeze this

5) supervised late game evaluation - take late game states, calculate the expectation precisely
    for 1 2 3 missing, or using MCTS for 4 5 missing, and keep this all the time in the learning buffer

6) for pure MCTS, reduce the number of simulations as the game progresses
7) use symmetries to find the classes of equivalent games in the early stages of search - easy way: generate all symmetries for each position, hash them - in get possible moves

8) instead of E[X] it might be better to play against random and/or MCTS to evalaute per game difference - remove the randomness from game selection